{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "def lorenz(state, t, sigma=10, beta=8/3, rho=28):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x * (rho - z) - y\n",
    "    dzdt = x * y - beta * z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def simulate_chaos(t, initial_state):\n",
    "    return odeint(lorenz, initial_state, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "# Environment settings\n",
    "env_size = 10  # Size of the 2D environment\n",
    "\n",
    "# Targets\n",
    "targets = [\n",
    "    np.array([2, 4]),\n",
    "    np.array([6, 4]),\n",
    "    np.array([6, 1])\n",
    "]\n",
    "\n",
    "# Obstacles\n",
    "obstacles = [\n",
    "    np.array([4, 4]),\n",
    "    np.array([6, 2])\n",
    "]\n",
    "\n",
    "# Agent settings\n",
    "agent_position = np.array([5.0, 5.0])\n",
    "agent_velocity = np.array([0.0, 0.0])\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Simulation parameters\n",
    "time_steps = 5000\n",
    "dt = 0.1\n",
    "chaos_weight = 0.5\n",
    "chaos_decay = 0.55\n",
    "chaos_state = [1.0, 1.0, 1.0]\n",
    "\n",
    "# Multi-target prioritization settings\n",
    "# Options: 'closest', 'furthest', 'sequence'\n",
    "search_strategy = 'sequence'  # User can choose 'closest', 'furthest', or 'sequence'\n",
    "target_sequence = [0,1,2]   # Indices of targets in the order to visit (used if 'sequence' is selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "def reward(agent_position, current_target):\n",
    "    distance_to_target = np.linalg.norm(agent_position - current_target)\n",
    "    if distance_to_target <= 1.5:\n",
    "        return 1  # High reward near target\n",
    "    return -0.1  # Penalize being away from the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "def select_target(agent_position, targets, strategy, sequence, step):\n",
    "    if strategy == 'closest':\n",
    "        distances = [np.linalg.norm(agent_position - t) for t in targets]\n",
    "        return targets[np.argmin(distances)]\n",
    "    elif strategy == 'furthest':\n",
    "        distances = [np.linalg.norm(agent_position - t) for t in targets]\n",
    "        return targets[np.argmax(distances)]\n",
    "    elif strategy == 'sequence':\n",
    "        index = step // (time_steps // len(sequence))\n",
    "        index = min(index, len(sequence) - 1)\n",
    "        return targets[sequence[index]]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid search strategy selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "# Visualization setup\n",
    "trajectory = [agent_position.copy()]\n",
    "reward_history = []\n",
    "visited_targets = set()\n",
    "current_target = None\n",
    "\n",
    "for step in range(time_steps):\n",
    "    # Select current target\n",
    "    current_target = select_target(agent_position, targets, search_strategy, target_sequence, step)\n",
    "    \n",
    "    # Check if the target has been reached\n",
    "    if np.linalg.norm(agent_position - current_target) <= 1.5:\n",
    "        visited_targets.add(tuple(current_target))\n",
    "    \n",
    "    # Calculate reward\n",
    "    current_reward = reward(agent_position, current_target)\n",
    "    reward_history.append(current_reward)\n",
    "    \n",
    "    # Calculate gradient towards target\n",
    "    gradient = current_target - agent_position\n",
    "    gradient_norm = np.linalg.norm(gradient)\n",
    "    if gradient_norm > 0:\n",
    "        gradient /= gradient_norm  # Normalize gradient\n",
    "    \n",
    "    # Chaotic dynamics contribution\n",
    "    t = np.linspace(0, dt, 10)\n",
    "    chaos_state = simulate_chaos(t, chaos_state)[-1]\n",
    "    chaotic_force = chaos_state[:2]  # Use x and y components\n",
    "    \n",
    "    # Update agent's velocity and position\n",
    "    agent_velocity = learning_rate * gradient + chaos_weight * chaotic_force\n",
    "    agent_position += agent_velocity * dt\n",
    "    trajectory.append(agent_position.copy())\n",
    "    \n",
    "    # Decay chaotic contribution\n",
    "    chaos_weight *= chaos_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1], label=\"Trajectory\", linewidth=1.5)\n",
    "plt.scatter(*np.array([t for t in targets]).T, color=\"green\", label=\"Targets\", s=100)\n",
    "plt.scatter(*np.array(obstacles).T, color=\"red\", label=\"Obstacles\", s=100)\n",
    "plt.title(\"Agent Trajectory with Chaotic Dynamics\")\n",
    "plt.xlim(0, env_size)\n",
    "plt.ylim(0, env_size)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Cell\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(reward_history, label=\"Reward\")\n",
    "plt.title(\"Reward Over Time\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
