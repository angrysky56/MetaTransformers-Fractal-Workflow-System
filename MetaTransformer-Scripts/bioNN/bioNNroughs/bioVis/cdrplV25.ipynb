{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import random\n",
    "\n",
    "# Environment and Agent Setup\n",
    "class AgentEnvironment:\n",
    "    def __init__(self, num_targets=5, num_obstacles=3):\n",
    "        self.agent_pos = np.array([0.0, 0.0])\n",
    "        self.targets = [{\"pos\": np.random.rand(2) * 10, \"weight\": random.randint(1, 10)} for _ in range(num_targets)]\n",
    "        self.obstacles = [np.random.rand(2) * 10 for _ in range(num_obstacles)]\n",
    "        self.learning_rate = 0.05\n",
    "        self.chaos = 0.2\n",
    "        self.chaos_decay = 0.55\n",
    "\n",
    "    def step(self):\n",
    "        # Calculate target priorities\n",
    "        target_scores = [\n",
    "            t[\"weight\"] / (np.linalg.norm(t[\"pos\"] - self.agent_pos) + 1e-3)\n",
    "            for t in self.targets\n",
    "        ]\n",
    "        # Select highest priority target\n",
    "        best_target_idx = np.argmax(target_scores)\n",
    "        target_pos = self.targets[best_target_idx][\"pos\"]\n",
    "\n",
    "        # Move agent toward target\n",
    "        direction = target_pos - self.agent_pos\n",
    "        step = self.learning_rate * direction + self.chaos * np.random.randn(2)\n",
    "        self.agent_pos += step\n",
    "\n",
    "        # Check for target reach\n",
    "        if np.linalg.norm(self.agent_pos - target_pos) < 0.5:\n",
    "            self.targets.pop(best_target_idx)  # Remove target once reached\n",
    "\n",
    "        # Reduce chaos over time\n",
    "        self.chaos *= self.chaos_decay\n",
    "\n",
    "    def run(self, steps=100):\n",
    "        trajectory = [self.agent_pos.copy()]\n",
    "        for _ in range(steps):\n",
    "            if not self.targets:\n",
    "                break\n",
    "            self.step()\n",
    "            trajectory.append(self.agent_pos.copy())\n",
    "        return np.array(trajectory)\n",
    "\n",
    "# Visualization\n",
    "def plot_environment(env, trajectory):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(trajectory[:, 0], trajectory[:, 1], label=\"Trajectory\")\n",
    "    for target in env.targets:\n",
    "        plt.scatter(*target[\"pos\"], c=\"green\", s=100 * target[\"weight\"], label=\"Target\")\n",
    "    for obstacle in env.obstacles:\n",
    "        plt.scatter(*obstacle, c=\"red\", s=100, label=\"Obstacle\")\n",
    "    plt.scatter(*trajectory[0], c=\"blue\", label=\"Start\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize Environment\n",
    "env = AgentEnvironment(num_targets=10, num_obstacles=5)\n",
    "trajectory = env.run(steps=200)\n",
    "plot_environment(env, trajectory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
